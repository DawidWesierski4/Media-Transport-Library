commit b06d5a240982d9c6961819026ac706d5ebd0313d
Author: Marek Kasiewicz <marek.kasiewicz@intel.com>
Date:   Tue May 20 10:04:59 2025 +0200

    Fix: RL pacing on DPDK 25.03 (#1148)
    
    - Use an eight-level hierarchical schedule for PF
    - Don't use shapers for non-leaf nodes, as thier rules are now correctly
    inherated to leaf nodes
    - Stop calling rte_eth_rx_burst() while rte_tm_hierarchy_commit() is in
    progress, as it causes error logs.
    - Rename dev_rl_init_root to dev_rl_init_nonleaf_nodes and move it out
    of dev_rl_shaper_add
    
    ---------
    
    Signed-off-by: Kasiewicz, Marek <marek.kasiewicz@intel.com>
    
    Add: handling case where RL is lower then expected
    
    Signed-off-by: Kasiewicz, Marek <marek.kasiewicz@intel.com>
    
    Fix: seting rl rate on VF
    
    Signed-off-by: Kasiewicz, Marek <marek.kasiewicz@intel.com>
    
    Update lib/src/st2110/st_tx_video_session.c
    
    Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>
    
    Fix: protect shared port access with proper sync
    
    Signed-off-by: Kasiewicz, Marek <marek.kasiewicz@intel.com>

diff --git a/lib/src/datapath/mt_queue.c b/lib/src/datapath/mt_queue.c
index 7cdf61bf..08765d5b 100644
--- a/lib/src/datapath/mt_queue.c
+++ b/lib/src/datapath/mt_queue.c
@@ -45,7 +45,20 @@ static uint16_t rx_csq_burst(struct mt_rxq_entry* entry, struct rte_mbuf** rx_pk
 
 static uint16_t rx_dpdk_burst(struct mt_rxq_entry* entry, struct rte_mbuf** rx_pkts,
                               const uint16_t nb_pkts) {
-  return mt_dpdk_rx_burst(entry->rxq, rx_pkts, nb_pkts);
+  enum mtl_port port_id = entry->rxq->port;
+  struct mt_interface* inf = mt_if(entry->parent, port_id);
+  int ret;
+
+  /* Trylock as we should not block in tasklates */
+  ret = mt_pthread_rwlock_tryrdlock(&inf->rl_rwlock);
+  if (ret) {
+    dbg("%s(%d), try lock fail %d\n", __func__, port_id, ret);
+    return 0;
+  }
+  ret = mt_dpdk_rx_burst(entry->rxq, rx_pkts, nb_pkts);
+  mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+
+  return ret;
 }
 
 struct mt_rxq_entry* mt_rxq_get(struct mtl_main_impl* impl, enum mtl_port port,
@@ -163,7 +176,20 @@ static uint16_t tx_tsq_burst(struct mt_txq_entry* entry, struct rte_mbuf** tx_pk
 
 static uint16_t tx_dpdk_burst(struct mt_txq_entry* entry, struct rte_mbuf** tx_pkts,
                               uint16_t nb_pkts) {
-  return mt_dpdk_tx_burst(entry->txq, tx_pkts, nb_pkts);
+  enum mtl_port port_id = entry->txq->port;
+  struct mt_interface* inf = mt_if(entry->parent, port_id);
+  uint16_t ret;
+
+  /* Trylock as we should not block in tasklates */
+  ret = mt_pthread_rwlock_tryrdlock(&inf->rl_rwlock);
+  if (ret) {
+    dbg("%s(%d), try lock fail %d\n", __func__, port_id, ret);
+    return 0;
+  }
+  ret = mt_dpdk_tx_burst(entry->txq, tx_pkts, nb_pkts);
+  mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+
+  return ret;
 }
 
 struct mt_txq_entry* mt_txq_get(struct mtl_main_impl* impl, enum mtl_port port,
diff --git a/lib/src/dev/mt_dev.c b/lib/src/dev/mt_dev.c
index 32a2278d..a266d17c 100644
--- a/lib/src/dev/mt_dev.c
+++ b/lib/src/dev/mt_dev.c
@@ -550,38 +550,43 @@ static int dev_flush_rx_queue(struct mt_interface* inf, struct mt_rx_queue* queu
 
 #define ST_SHAPER_PROFILE_ID 1
 #define ST_ROOT_NODE_ID 256
-#define ST_DEFAULT_NODE_ID 246
+#define ST_TM_NONLEAF_NODES_NUM_PF 7
+#define ST_TM_NONLEAF_NODES_NUM_VF 2
+#define ST_TM_LAST_NONLEAF_NODE_ID_VF (ST_ROOT_NODE_ID + ST_TM_NONLEAF_NODES_NUM_VF - 1)
+#define ST_TM_LAST_NONLEAF_NODE_ID_PF (ST_ROOT_NODE_ID + ST_TM_NONLEAF_NODES_NUM_PF - 1)
 #define ST_DEFAULT_RL_BPS (1024 * 1024 * 1024 / 8) /* 1g bit per second */
 
-static int dev_rl_init_root(struct mt_interface* inf, uint32_t shaper_profile_id) {
+static int dev_rl_init_nonleaf_nodes(struct mt_interface* inf) {
   uint16_t port_id = inf->port_id;
   enum mtl_port port = inf->port;
   int ret;
   struct rte_tm_error error;
   struct rte_tm_node_params np;
+  uint32_t parent_id = RTE_TM_NODE_ID_NULL;
+  uint32_t node_id = ST_ROOT_NODE_ID;
+  uint32_t nonleaf_nodes_num;
 
   if (inf->tx_rl_root_active) return 0;
 
   memset(&error, 0, sizeof(error));
   memset(&np, 0, sizeof(np));
 
-  /* root node */
-  np.shaper_profile_id = shaper_profile_id;
+  np.shaper_profile_id = RTE_TM_SHAPER_PROFILE_ID_NONE;
   np.nonleaf.n_sp_priorities = 1;
-  ret = rte_tm_node_add(port_id, ST_ROOT_NODE_ID, -1, 0, 1, 0, &np, &error);
-  if (ret < 0) {
-    err("%s(%d), root add error: (%d)%s\n", __func__, port, ret,
-        mt_string_safe(error.message));
-    return ret;
-  }
+  if (inf->drv_info.drv_type == MT_DRV_IAVF)
+    nonleaf_nodes_num = ST_TM_NONLEAF_NODES_NUM_VF;
+  else
+    nonleaf_nodes_num = ST_TM_NONLEAF_NODES_NUM_PF;
 
-  /* nonleaf node based on root */
-  ret =
-      rte_tm_node_add(port_id, ST_DEFAULT_NODE_ID, ST_ROOT_NODE_ID, 0, 1, 1, &np, &error);
-  if (ret < 0) {
-    err("%s(%d), node add error: (%d)%s\n", __func__, port, ret,
-        mt_string_safe(error.message));
-    return ret;
+  for (int i = 0; i < nonleaf_nodes_num; i++) {
+    node_id = ST_ROOT_NODE_ID + i;
+    ret = rte_tm_node_add(port_id, node_id, parent_id, 0, 1, i, &np, &error);
+    if (ret < 0) {
+      err("%s(%d), node add error: (%d)%s\n", __func__, port, ret,
+          mt_string_safe(error.message));
+      return ret;
+    }
+    parent_id = node_id;
   }
 
   inf->tx_rl_root_active = true;
@@ -614,13 +619,6 @@ static struct mt_rl_shaper* dev_rl_shaper_add(struct mt_interface* inf, uint64_t
       return NULL;
     }
 
-    ret = dev_rl_init_root(inf, shaper_profile_id);
-    if (ret < 0) {
-      err("%s(%d), root init error %d\n", __func__, port, ret);
-      rte_tm_shaper_profile_delete(port_id, shaper_profile_id, &error);
-      return NULL;
-    }
-
     info("%s(%d), bps %" PRIu64 " on shaper %d\n", __func__, port, bps,
          shaper_profile_id);
     shapers[i].rl_bps = bps;
@@ -646,15 +644,15 @@ static struct mt_rl_shaper* dev_rl_shaper_get(struct mt_interface* inf, uint64_t
 static int dev_init_ratelimit_all(struct mt_interface* inf) {
   uint16_t port_id = inf->port_id;
   enum mtl_port port = inf->port;
-  int ret;
   struct rte_tm_error error;
   struct rte_tm_node_params qp;
+  struct mt_tx_queue* tx_queue;
   struct mt_rl_shaper* shaper;
   uint64_t bps = ST_DEFAULT_RL_BPS;
+  int ret;
 
   memset(&error, 0, sizeof(error));
 
-  struct mt_tx_queue* tx_queue;
   for (uint16_t q = 0; q < inf->nb_tx_q; q++) {
     tx_queue = &inf->tx_queues[q];
 
@@ -667,7 +665,13 @@ static int dev_init_ratelimit_all(struct mt_interface* inf) {
     qp.shaper_profile_id = shaper->shaper_profile_id;
     qp.leaf.cman = RTE_TM_CMAN_TAIL_DROP;
     qp.leaf.wred.wred_profile_id = RTE_TM_WRED_PROFILE_ID_NONE;
-    ret = rte_tm_node_add(port_id, q, ST_DEFAULT_NODE_ID, 0, 1, 2, &qp, &error);
+    if (inf->drv_info.drv_type == MT_DRV_IAVF) {
+      ret = rte_tm_node_add(port_id, q, ST_TM_LAST_NONLEAF_NODE_ID_VF, 0, 1,
+                            ST_TM_NONLEAF_NODES_NUM_VF, &qp, &error);
+    } else {
+      ret = rte_tm_node_add(port_id, q, ST_TM_LAST_NONLEAF_NODE_ID_PF, 0, 1,
+                            ST_TM_NONLEAF_NODES_NUM_PF, &qp, &error);
+    }
     if (ret < 0) {
       err("%s(%d), q %d add fail %d(%s)\n", __func__, port, q, ret,
           mt_string_safe(error.message));
@@ -679,13 +683,8 @@ static int dev_init_ratelimit_all(struct mt_interface* inf) {
          shaper->shaper_profile_id);
   }
 
-  ret = rte_tm_hierarchy_commit(port_id, 1, &error);
-  if (ret < 0)
-    err("%s(%d), commit error (%d)%s\n", __func__, port, ret,
-        mt_string_safe(error.message));
-
   dbg("%s(%d), succ\n", __func__, port);
-  return ret;
+  return 0;
 }
 
 static int dev_tx_queue_set_rl_rate(struct mt_interface* inf, uint16_t queue,
@@ -708,13 +707,25 @@ static int dev_tx_queue_set_rl_rate(struct mt_interface* inf, uint16_t queue,
   /* not changed */
   if (bps == tx_queue->bps) return 0;
 
+  ret = mt_pthread_rwlock_wrlock(&inf->rl_rwlock);
+  if (ret) {
+    err("%s(%d), failed to acquire write lock, ret %d\n", __func__, port, ret);
+    return ret;
+  }
+
+  ret = rte_eth_dev_stop(port_id);
+  if (ret) {
+    err("%s(%d), stop port %d fail %d\n", __func__, port, port_id, ret);
+    goto exit;
+  }
+
   /* delete old queue node */
   if (tx_queue->rl_shapers_mapping >= 0) {
     ret = rte_tm_node_delete(port_id, queue, &error);
     if (ret < 0) {
       err("%s(%d), node %d delete fail %d(%s)\n", __func__, port, queue, ret,
           mt_string_safe(error.message));
-      return ret;
+      goto exit;
     }
     tx_queue->rl_shapers_mapping = -1;
   }
@@ -723,35 +734,79 @@ static int dev_tx_queue_set_rl_rate(struct mt_interface* inf, uint16_t queue,
     shaper = dev_rl_shaper_get(inf, bps);
     if (!shaper) {
       err("%s(%d), rl shaper get fail for q %d\n", __func__, port, queue);
-      return -EIO;
+      ret = -EIO;
+      goto exit;
     }
     memset(&qp, 0, sizeof(qp));
     qp.shaper_profile_id = shaper->shaper_profile_id;
     qp.leaf.cman = RTE_TM_CMAN_TAIL_DROP;
     qp.leaf.wred.wred_profile_id = RTE_TM_WRED_PROFILE_ID_NONE;
-    ret = rte_tm_node_add(port_id, queue, ST_DEFAULT_NODE_ID, 0, 1, 2, &qp, &error);
-    if (ret < 0) {
+    if (inf->drv_info.drv_type == MT_DRV_IAVF) {
+      ret = rte_tm_node_add(port_id, queue, ST_TM_LAST_NONLEAF_NODE_ID_VF, 0, 1,
+                            ST_TM_NONLEAF_NODES_NUM_VF, &qp, &error);
+    } else {
+      ret = rte_tm_node_add(port_id, queue, ST_TM_LAST_NONLEAF_NODE_ID_PF, 0, 1,
+                            ST_TM_NONLEAF_NODES_NUM_PF, &qp, &error);
+    }
+    if (ret) {
       err("%s(%d), q %d add fail %d(%s)\n", __func__, port, queue, ret,
           mt_string_safe(error.message));
-      return ret;
+      goto exit;
     }
+
     tx_queue->rl_shapers_mapping = shaper->idx;
     info("%s(%d), q %d link to shaper id %d(%" PRIu64 ")\n", __func__, port, queue,
          shaper->shaper_profile_id, shaper->rl_bps);
   }
 
-  mt_pthread_mutex_lock(&inf->vf_cmd_mutex);
-  ret = rte_tm_hierarchy_commit(port_id, 1, &error);
-  mt_pthread_mutex_unlock(&inf->vf_cmd_mutex);
-  if (ret < 0) {
-    err("%s(%d), commit error (%d)%s\n", __func__, port, ret,
-        mt_string_safe(error.message));
-    return ret;
+  if (inf->drv_info.drv_type == MT_DRV_IAVF) {
+    /* Note: IAVF behavior in DPDK 25.03:
+     * 1. If the hierarchy is committed on a started device, nodes cannot be removed after
+     * the commit.
+     * 2. After committing the hierarchy, the device must be restarted for changes to take
+     * effect.
+     */
+    ret = rte_eth_dev_start(port_id);
+    if (ret) {
+      err("%s(%d), start port %d fail %d\n", __func__, port, port_id, ret);
+      goto exit;
+    }
+
+    ret = rte_tm_hierarchy_commit(port_id, 1, &error);
+    if (ret) {
+      err("%s(%d), commit error (%d)%s\n", __func__, port, ret,
+          mt_string_safe(error.message));
+      goto exit;
+    }
+
+    ret = rte_eth_dev_stop(port_id);
+    if (ret) {
+      err("%s(%d), stop port %d fail %d\n", __func__, port, port_id, ret);
+      goto exit;
+    }
+
+  } else {
+    ret = rte_tm_hierarchy_commit(port_id, 1, &error);
+    if (ret) {
+      err("%s(%d), commit error (%d)%s\n", __func__, port, ret,
+          mt_string_safe(error.message));
+      goto exit;
+    }
+  }
+
+  ret = rte_eth_dev_start(port_id);
+  if (ret) {
+    err("%s(%d), start port %d fail %d\n", __func__, port, port_id, ret);
+    goto exit;
   }
 
   tx_queue->bps = bps;
 
-  return 0;
+exit:
+  ret = mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+  if (ret) err("%s(%d), failed to release write lock, ret %d\n", __func__, port, ret);
+
+  return ret;
 }
 
 static int dev_stop_port(struct mt_interface* inf) {
@@ -1403,14 +1458,17 @@ static int dev_if_init_pacing(struct mt_interface* inf) {
       /* detect done in the xdp pacing init already */
       return 0;
     }
+    ret = dev_rl_init_nonleaf_nodes(inf);
+    if (ret < 0) {
+      err("%s(%d), root init error %d\n", __func__, port, ret);
+      return ret;
+    }
     /* IAVF require all q config with RL */
-    if (inf->drv_info.drv_type == MT_DRV_IAVF) {
+    if (inf->drv_info.drv_type == MT_DRV_IAVF)
       ret = dev_init_ratelimit_all(inf);
-    } else {
+    else
       ret = dev_tx_queue_set_rl_rate(inf, 0, ST_DEFAULT_RL_BPS);
-      if (ret >= 0) dev_tx_queue_set_rl_rate(inf, 0, 0);
-    }
-    if (ret < 0) { /* fallback to tsc if no rl */
+    if (ret) { /* fallback to tsc if no rl */
       if (auto_detect) {
         warn("%s(%d), fallback to tsc as rl init fail\n", __func__, port);
         inf->tx_pacing_way = ST21_TX_PACING_WAY_TSC;
@@ -1535,7 +1593,7 @@ struct mt_tx_queue* mt_dev_get_tx_queue(struct mtl_main_impl* impl, enum mtl_por
     tx_queue = &inf->tx_queues[q];
     if (tx_queue->active || tx_queue->fatal_error) continue;
 
-    if (inf->tx_pacing_way == ST21_TX_PACING_WAY_RL) {
+    if (inf->tx_pacing_way == ST21_TX_PACING_WAY_RL && bytes_per_sec) {
       ret = dev_tx_queue_set_rl_rate(inf, q, bytes_per_sec);
       if (ret < 0) {
         err("%s(%d), fallback to tsc as rl fail\n", __func__, port);
@@ -2061,7 +2119,7 @@ int mt_dev_if_uinit(struct mtl_main_impl* impl) {
 
     mt_pthread_mutex_destroy(&inf->tx_queues_mutex);
     mt_pthread_mutex_destroy(&inf->rx_queues_mutex);
-    mt_pthread_mutex_destroy(&inf->vf_cmd_mutex);
+    mt_pthread_rwlock_destroy(&inf->rl_rwlock);
 
     dev_close_port(inf);
   }
@@ -2127,7 +2185,7 @@ int mt_dev_if_init(struct mtl_main_impl* impl) {
     inf->tx_pacing_way = p->pacing;
     mt_pthread_mutex_init(&inf->tx_queues_mutex, NULL);
     mt_pthread_mutex_init(&inf->rx_queues_mutex, NULL);
-    mt_pthread_mutex_init(&inf->vf_cmd_mutex, NULL);
+    mt_pthread_rwlock_pref_wr_init(&inf->rl_rwlock);
     rte_spinlock_init(&inf->stats_lock);
 
     if (mt_user_ptp_tsc_source(impl)) {
diff --git a/lib/src/mt_flow.c b/lib/src/mt_flow.c
index 676db196..b5129785 100644
--- a/lib/src/mt_flow.c
+++ b/lib/src/mt_flow.c
@@ -27,6 +27,7 @@ static struct rte_flow* rte_rx_flow_create_raw(struct mt_interface* inf, uint16_
   struct rte_flow_item_raw spec = {0};
   struct rte_flow_item_raw mask = {0};
   struct rte_flow_action_queue to_queue = {0};
+  int ret;
 
   uint16_t port_id = inf->port_id;
   char pkt_buf[] =
@@ -58,9 +59,17 @@ static struct rte_flow* rte_rx_flow_create_raw(struct mt_interface* inf, uint16_
   action[0].conf = &to_queue;
   action[1].type = RTE_FLOW_ACTION_TYPE_END;
 
-  mt_pthread_mutex_lock(&inf->vf_cmd_mutex);
+  ret = mt_pthread_rwlock_wrlock(&inf->rl_rwlock);
+  if (ret) {
+    err("%s(%d), failed to acquire write lock, ret %d\n", __func__, port_id, ret);
+    return NULL;
+  }
   r_flow = rte_flow_create(port_id, &attr, pattern, action, &error);
-  mt_pthread_mutex_unlock(&inf->vf_cmd_mutex);
+  ret = mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+  if (ret) {
+    err("%s(%d), failed to release write lock, ret %d\n", __func__, port_id, ret);
+    return NULL;
+  }
   if (!r_flow) {
     err("%s(%d), rte_flow_create fail for queue %d, %s\n", __func__, port_id, q,
         mt_string_safe(error.message));
@@ -175,9 +184,17 @@ static struct rte_flow* rte_rx_flow_create(struct mt_interface* inf, uint16_t q,
     return NULL;
   }
 
-  mt_pthread_mutex_lock(&inf->vf_cmd_mutex);
+  ret = mt_pthread_rwlock_wrlock(&inf->rl_rwlock);
+  if (ret) {
+    err("%s(%d), failed to acquire write lock, ret %d\n", __func__, port_id, ret);
+    return NULL;
+  }
   r_flow = rte_flow_create(port_id, &attr, pattern, action, &error);
-  mt_pthread_mutex_unlock(&inf->vf_cmd_mutex);
+  ret = mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+  if (ret) {
+    err("%s(%d), failed to release write lock, ret %d\n", __func__, port_id, ret);
+    return NULL;
+  }
 
   /* WA specific for e810 for PF interfaces */
   if (!has_ip_flow && !r_flow) {
@@ -192,9 +209,17 @@ static struct rte_flow* rte_rx_flow_create(struct mt_interface* inf, uint16_t q,
       return NULL;
     }
 
-    mt_pthread_mutex_lock(&inf->vf_cmd_mutex);
+    ret = mt_pthread_rwlock_wrlock(&inf->rl_rwlock);
+    if (ret) {
+      err("%s(%d), failed to acquire write lock, ret %d\n", __func__, port_id, ret);
+      return NULL;
+    }
     r_flow = rte_flow_create(port_id, &attr, pattern, action, &error);
-    mt_pthread_mutex_unlock(&inf->vf_cmd_mutex);
+    ret = mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+    if (ret) {
+      err("%s(%d), failed to release write lock, ret %d\n", __func__, port_id, ret);
+      return NULL;
+    }
   }
 
   if (!r_flow) {
@@ -264,6 +289,7 @@ static int rx_flow_free(struct mt_interface* inf, struct mt_rx_flow_rsp* rsp) {
   enum mtl_port port = inf->port;
   struct rte_flow_error error;
   int ret;
+  int rwlock_ret;
   int max_retry = 5;
   int retry = 0;
 
@@ -273,9 +299,18 @@ retry:
     rsp->flow_id = -1;
   }
   if (rsp->flow) {
-    mt_pthread_mutex_lock(&inf->vf_cmd_mutex);
+    rwlock_ret = mt_pthread_rwlock_wrlock(&inf->rl_rwlock);
+    if (rwlock_ret) {
+      err("%s(%d), failed to acquire write lock, ret %d\n", __func__, port, rwlock_ret);
+      return rwlock_ret;
+    }
     ret = rte_flow_destroy(inf->port_id, rsp->flow, &error);
-    mt_pthread_mutex_unlock(&inf->vf_cmd_mutex);
+    rwlock_ret = mt_pthread_rwlock_unlock(&inf->rl_rwlock);
+    if (rwlock_ret) {
+      err("%s(%d), failed to release write lock, ret %d\n", __func__, port, rwlock_ret);
+      return rwlock_ret;
+    }
+
     if (ret < 0) {
       err("%s(%d), flow destroy fail, queue %d, retry %d\n", __func__, port,
           rsp->queue_id, retry);
diff --git a/lib/src/mt_main.h b/lib/src/mt_main.h
index e0134863..84128659 100644
--- a/lib/src/mt_main.h
+++ b/lib/src/mt_main.h
@@ -616,13 +616,9 @@ struct mt_sch_mgr {
   enum mt_lcore_type local_lcores_type[RTE_MAX_LCORE];
 };
 
-struct mt_audio_pacing_train_result {
-  uint64_t input_bps;    /* input, byte per sec */
-  uint64_t profiled_bps; /* profiled result */
-};
-
 struct mt_pacing_train_result {
-  uint64_t rl_bps;           /* input, byte per sec */
+  uint64_t input_bps;        /* input, byte per sec */
+  uint64_t profiled_bps;     /* profiled result */
   float pacing_pad_interval; /* result */
 };
 
@@ -717,11 +713,9 @@ struct mt_interface {
   uint16_t nb_rx_desc;
 
   struct rte_mbuf* pad;
-  /*
-   * protect rl and fdir for vf.
-   * _atomic_set_cmd(): There is incomplete cmd 112
-   */
-  pthread_mutex_t vf_cmd_mutex;
+
+  /* protect port during reset when changing RL speed */
+  pthread_rwlock_t rl_rwlock;
 
   /* tx queue resources */
   uint16_t nb_tx_q;
@@ -740,8 +734,6 @@ struct mt_interface {
   bool tx_rl_root_active;
   /* video rl pacing train result */
   struct mt_pacing_train_result pt_results[MT_MAX_RL_ITEMS];
-  /* audio rl pacing train result */
-  struct mt_audio_pacing_train_result audio_pt_results[MT_MAX_RL_ITEMS];
 
   /* function ops per interface(pf/vf) */
   uint64_t (*ptp_get_time_fn)(struct mtl_main_impl* impl, enum mtl_port port);
diff --git a/lib/src/mt_platform.h b/lib/src/mt_platform.h
index e0061bc5..579b4856 100644
--- a/lib/src/mt_platform.h
+++ b/lib/src/mt_platform.h
@@ -93,6 +93,55 @@ static inline int mt_pthread_mutex_destroy(pthread_mutex_t* mutex) {
   return pthread_mutex_destroy(mutex);
 }
 
+static inline int mt_pthread_rwlock_init(pthread_rwlock_t* rwlock,
+                                         pthread_rwlockattr_t* attr) {
+  return pthread_rwlock_init(rwlock, attr);
+}
+
+static inline int mt_pthread_rwlock_pref_wr_init(pthread_rwlock_t* rwlock) {
+  pthread_rwlockattr_t rwlock_attr;
+  int ret;
+
+  ret = pthread_rwlockattr_init(&rwlock_attr);
+  if (ret) return ret;
+
+  ret = pthread_rwlockattr_setkind_np(&rwlock_attr,
+                                      PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP);
+  if (ret) {
+    pthread_rwlockattr_destroy(&rwlock_attr);
+    return ret;
+  }
+
+  ret = pthread_rwlock_init(rwlock, &rwlock_attr);
+  pthread_rwlockattr_destroy(&rwlock_attr);
+
+  return ret;
+}
+
+static inline int mt_pthread_rwlock_rdlock(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_rdlock(rwlock);
+}
+
+static inline int mt_pthread_rwlock_tryrdlock(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_tryrdlock(rwlock);
+}
+
+static inline int mt_pthread_rwlock_wrlock(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_wrlock(rwlock);
+}
+
+static inline int mt_pthread_rwlock_trywrlock(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_trywrlock(rwlock);
+}
+
+static inline int mt_pthread_rwlock_unlock(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_unlock(rwlock);
+}
+
+static inline int mt_pthread_rwlock_destroy(pthread_rwlock_t* rwlock) {
+  return pthread_rwlock_destroy(rwlock);
+}
+
 static inline int mt_pthread_cond_init(pthread_cond_t* cond,
                                        pthread_condattr_t* cond_attr) {
   return pthread_cond_init(cond, cond_attr);
diff --git a/lib/src/mt_util.c b/lib/src/mt_util.c
index ecd16fe4..cabd05b7 100644
--- a/lib/src/mt_util.c
+++ b/lib/src/mt_util.c
@@ -251,13 +251,13 @@ int mt_build_port_map(struct mtl_main_impl* impl, char** ports, enum mtl_port* m
   return 0;
 }
 
-int mt_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port port,
-                               uint64_t rl_bps, float pad_interval) {
+int mt_pacing_train_pad_result_add(struct mtl_main_impl* impl, enum mtl_port port,
+                                   uint64_t input_bps, float pad_interval) {
   struct mt_pacing_train_result* ptr = &mt_if(impl, port)->pt_results[0];
 
   for (int i = 0; i < MT_MAX_RL_ITEMS; i++) {
-    if (ptr[i].rl_bps) continue;
-    ptr[i].rl_bps = rl_bps;
+    if (ptr[i].input_bps) continue;
+    ptr[i].input_bps = input_bps;
     ptr[i].pacing_pad_interval = pad_interval;
     return 0;
   }
@@ -266,12 +266,12 @@ int mt_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port port,
   return -ENOMEM;
 }
 
-int mt_pacing_train_result_search(struct mtl_main_impl* impl, enum mtl_port port,
-                                  uint64_t rl_bps, float* pad_interval) {
+int mt_pacing_train_pad_result_search(struct mtl_main_impl* impl, enum mtl_port port,
+                                      uint64_t rl_bps, float* pad_interval) {
   struct mt_pacing_train_result* ptr = &mt_if(impl, port)->pt_results[0];
 
   for (int i = 0; i < MT_MAX_RL_ITEMS; i++) {
-    if (rl_bps == ptr[i].rl_bps) {
+    if (rl_bps == ptr[i].input_bps && ptr[i].pacing_pad_interval) {
       *pad_interval = ptr[i].pacing_pad_interval;
       return 0;
     }
@@ -281,9 +281,9 @@ int mt_pacing_train_result_search(struct mtl_main_impl* impl, enum mtl_port port
   return -EINVAL;
 }
 
-int mt_audio_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port port,
-                                     uint64_t input_bps, uint64_t profiled_bps) {
-  struct mt_audio_pacing_train_result* ptr = &mt_if(impl, port)->audio_pt_results[0];
+int mt_pacing_train_bps_result_add(struct mtl_main_impl* impl, enum mtl_port port,
+                                   uint64_t input_bps, uint64_t profiled_bps) {
+  struct mt_pacing_train_result* ptr = &mt_if(impl, port)->pt_results[0];
 
   for (int i = 0; i < MT_MAX_RL_ITEMS; i++) {
     if (ptr[i].input_bps) continue;
@@ -296,12 +296,12 @@ int mt_audio_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port p
   return -ENOMEM;
 }
 
-int mt_audio_pacing_train_result_search(struct mtl_main_impl* impl, enum mtl_port port,
-                                        uint64_t input_bps, uint64_t* profiled_bps) {
-  struct mt_audio_pacing_train_result* ptr = &mt_if(impl, port)->audio_pt_results[0];
+int mt_pacing_train_bps_result_search(struct mtl_main_impl* impl, enum mtl_port port,
+                                      uint64_t input_bps, uint64_t* profiled_bps) {
+  struct mt_pacing_train_result* ptr = &mt_if(impl, port)->pt_results[0];
 
   for (int i = 0; i < MT_MAX_RL_ITEMS; i++) {
-    if (input_bps == ptr[i].input_bps) {
+    if (input_bps == ptr[i].input_bps && ptr[i].profiled_bps) {
       *profiled_bps = ptr[i].profiled_bps;
       return 0;
     }
diff --git a/lib/src/mt_util.h b/lib/src/mt_util.h
index 362c8650..1659744d 100644
--- a/lib/src/mt_util.h
+++ b/lib/src/mt_util.h
@@ -56,15 +56,15 @@ int mt_ring_dequeue_clean(struct rte_ring* ring);
 
 void mt_mbuf_sanity_check(struct rte_mbuf** mbufs, uint16_t nb, char* tag);
 
-int mt_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port port,
-                               uint64_t rl_bps, float pad_interval);
-int mt_pacing_train_result_search(struct mtl_main_impl* impl, enum mtl_port port,
-                                  uint64_t rl_bps, float* pad_interval);
-
-int mt_audio_pacing_train_result_add(struct mtl_main_impl* impl, enum mtl_port port,
-                                     uint64_t input_bps, uint64_t profiled_bps);
-int mt_audio_pacing_train_result_search(struct mtl_main_impl* impl, enum mtl_port port,
-                                        uint64_t input_bps, uint64_t* profiled_bps);
+int mt_pacing_train_pad_result_add(struct mtl_main_impl* impl, enum mtl_port port,
+                                   uint64_t input_bps, float pad_interval);
+int mt_pacing_train_pad_result_search(struct mtl_main_impl* impl, enum mtl_port port,
+                                      uint64_t input_bps, float* pad_interval);
+
+int mt_pacing_train_bps_result_add(struct mtl_main_impl* impl, enum mtl_port port,
+                                   uint64_t input_bps, uint64_t profiled_bps);
+int mt_pacing_train_bps_result_search(struct mtl_main_impl* impl, enum mtl_port port,
+                                      uint64_t input_bps, uint64_t* profiled_bps);
 
 enum mtl_port mt_port_by_name(struct mtl_main_impl* impl, const char* name);
 int mt_build_port_map(struct mtl_main_impl* impl, char** ports, enum mtl_port* maps,
diff --git a/lib/src/st2110/st_tx_audio_session.c b/lib/src/st2110/st_tx_audio_session.c
index cb7a35f7..e96aff52 100644
--- a/lib/src/st2110/st_tx_audio_session.c
+++ b/lib/src/st2110/st_tx_audio_session.c
@@ -1251,8 +1251,8 @@ static int tx_audio_session_init_rl(struct mtl_main_impl* impl,
     port = mt_port_logic2phy(s->port_maps, i);
 
     uint64_t initial_bytes_per_sec = tx_audio_session_initial_rl_bps(s);
-    int profiled = mt_audio_pacing_train_result_search(impl, port, initial_bytes_per_sec,
-                                                       &profiled_per_sec);
+    int profiled = mt_pacing_train_bps_result_search(impl, port, initial_bytes_per_sec,
+                                                     &profiled_per_sec);
 
     /* pad pkt */
     rl_port->pad = mt_build_pad(impl, mt_sys_tx_mempool(impl, port), port,
@@ -1291,7 +1291,7 @@ static int tx_audio_session_init_rl(struct mtl_main_impl* impl,
           return -EIO;
         }
 
-        mt_audio_pacing_train_result_add(impl, port, initial_bytes_per_sec, trained);
+        mt_pacing_train_bps_result_add(impl, port, initial_bytes_per_sec, trained);
         info("%s(%d), trained bytes_per_sec %" PRIu64 "\n", __func__, idx, trained);
         int ret = mt_txq_set_tx_bps(rl_port->queue[j], trained);
         if (ret < 0) {
diff --git a/lib/src/st2110/st_tx_video_session.c b/lib/src/st2110/st_tx_video_session.c
index ce8672d4..90d5219a 100644
--- a/lib/src/st2110/st_tx_video_session.c
+++ b/lib/src/st2110/st_tx_video_session.c
@@ -322,6 +322,8 @@ static int tv_train_pacing(struct mtl_main_impl* impl, struct st_tx_video_sessio
   float pad_interval;
   uint64_t rl_bps = tv_rl_bps(s);
   uint64_t train_start_time, train_end_time;
+  double measured_bps;
+  uint64_t bps_to_set;
 
   uint16_t resolved = s->ops.pad_interval;
   if (resolved) {
@@ -338,7 +340,7 @@ static int tv_train_pacing(struct mtl_main_impl* impl, struct st_tx_video_sessio
     }
   }
 
-  ret = mt_pacing_train_result_search(impl, port, rl_bps, &pad_interval);
+  ret = mt_pacing_train_pad_result_search(impl, port, rl_bps, &pad_interval);
   if (ret >= 0) {
     s->pacing.pad_interval = pad_interval;
     info("%s(%d), use pre-train pad_interval %f\n", __func__, idx, pad_interval);
@@ -423,10 +425,24 @@ static int tv_train_pacing(struct mtl_main_impl* impl, struct st_tx_video_sessio
     reactive = (s->ops.height == 480) ? 487.0 / 525.0 : 576.0 / 625.0;
   }
   pkts_per_frame = pkts_per_frame * reactive;
-  if (pkts_per_frame < s->st20_total_pkts) {
-    err("%s(%d), error pkts_per_frame %f, st20_total_pkts %d\n", __func__, idx,
-        pkts_per_frame, s->st20_total_pkts);
-    return -EINVAL;
+  measured_bps = s->st20_pkt_size * pkts_per_sec * reactive;
+
+  /* If the measured speed is lower than expected. Set higher bps and retrain to add
+   * padding */
+  if (measured_bps < rl_bps) {
+    info("%s(%d), measured bps %" PRIu64 " is lower than set bps %" PRIu64 "\n", __func__,
+         idx, (uint64_t)measured_bps, rl_bps);
+    if (!mt_pacing_train_bps_result_search(impl, port, rl_bps, &bps_to_set)) {
+      err("%s(%d), measured speed is too low on already trained bps\n", __func__, idx);
+      return -EINVAL;
+    }
+
+    bps_to_set = (rl_bps * rl_bps) / measured_bps;
+    info("%s(%d), increase bps to %" PRIu64 "\n", __func__, idx, bps_to_set);
+    mt_pacing_train_bps_result_add(impl, port, rl_bps, bps_to_set);
+    mt_txq_set_tx_bps(queue, bps_to_set);
+    ret = tv_train_pacing(impl, s, s_port);
+    return ret;
   }
 
   pad_interval = (float)s->st20_total_pkts / (pkts_per_frame - s->st20_total_pkts);
@@ -437,7 +453,7 @@ static int tv_train_pacing(struct mtl_main_impl* impl, struct st_tx_video_sessio
   }
 
   s->pacing.pad_interval = pad_interval;
-  mt_pacing_train_result_add(impl, port, rl_bps, pad_interval);
+  mt_pacing_train_pad_result_add(impl, port, rl_bps, pad_interval);
   train_end_time = mt_get_tsc(impl);
   info("%s(%d,%d), trained pad_interval %f pkts_per_frame %f with time %fs\n", __func__,
        idx, s_port, pad_interval, pkts_per_frame,
@@ -2526,6 +2542,7 @@ static int tv_init_hw(struct mtl_main_impl* impl, struct st_tx_video_sessions_mg
     struct mt_txq_flow flow;
     memset(&flow, 0, sizeof(flow));
     flow.bytes_per_sec = tv_rl_bps(s);
+    mt_pacing_train_bps_result_search(impl, i, flow.bytes_per_sec, &flow.bytes_per_sec);
     mtl_memcpy(&flow.dip_addr, &s->ops.dip_addr[i], MTL_IP_ADDR_LEN);
     flow.dst_port = s->ops.udp_port[i];
     if (ST21_TX_PACING_WAY_TSN == s->pacing_way[i])
@@ -3858,6 +3875,8 @@ int st20_tx_queue_fatal_error(struct mtl_main_impl* impl,
   struct mt_txq_flow flow;
   memset(&flow, 0, sizeof(flow));
   flow.bytes_per_sec = tv_rl_bps(s);
+  mt_pacing_train_bps_result_search(impl, s_port, flow.bytes_per_sec,
+                                    &flow.bytes_per_sec);
   mtl_memcpy(&flow.dip_addr, &s->ops.dip_addr[s_port], MTL_IP_ADDR_LEN);
   flow.dst_port = s->ops.udp_port[s_port];
   s->queue[s_port] = mt_txq_get(impl, port, &flow);
